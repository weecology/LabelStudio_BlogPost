{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using label-studio for rapid annotation of geospatial data for airborne environmental monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environmental remote sensing using airborne image capture allows unprecedented insight into the distribution and abundance of organisms on earth. To detect and classify objects in airborne imagery, machine learning tools need to label the location and classes of objects. Here at the Weecology Lab at the University of Florida, we use label-studio to annotate airborne imagery for a variety of geospatial tasks, including tree detection in very high resolution orthophotos, and bird classification in UAV orthomosaics. Label-studio’s web interface has been key in creating labels for our machine learning workflow and keep our distributed team on the same page. Machine learning models for airborne object detection and classification require thousands of labels to create accurate detection models. In this post, I provide some key steps and uses for geospatial data labeling in label studio.\n",
    "\n",
    "1. Converting large geospatial tiles into smaller crops in a format compatible with label-studio?\n",
    "2. Creating a bounding box annotation project using the label-studio python-sdk\n",
    "3. Placing pre-annotated images into label-studio using existing foundation models\n",
    "\n",
    "The data comes from our work surveying bird colonies to support everglades restoration efforts using Unpiloted Aerial Vehivles (UAV). Let’s look at an orthomosaic image of one of those colonies. We also make use of our [DeepForest python package](), a tool for airborne model training focused on ecological monitoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1](public/Figure1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing geospatial data for labeling\n",
    "\n",
    "### Tiling orthomosaics into manageable pieces\n",
    "\n",
    "This orthomosaic has been georectified, meaning that the pixels in the imagery correspond to locations on the earth’s surface. While this is crucial for matching images among surveys, or associating images with other data products, it presents a couple challenges for standard annotation workflows. Geospatial data products are often very large, making them difficult to render and awkward to work with. Tiling the image into smaller pieces can be quite useful. Using our DeepForest python package, we have a couple helpful utility functions. These are mostly wrappers from common python packages for geospatial data, like Rasterio and Geopandas, which deserve a lot of credit in helping make geospatial data easy to use in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_raster() missing 1 required positional argument: 'annotations_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepforest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess\n\u001b[1;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/benweinstein/Dropbox/Weecology/everglades_species/Raw_versus_Ortho/Vulture_03_28_2022.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m crops \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_raster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_raster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(crops[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: split_raster() missing 1 required positional argument: 'annotations_file'"
     ]
    }
   ],
   "source": [
    "# pip install deepforest\n",
    "from deepforest import preprocess\n",
    "\n",
    "# see README.md for downloading data\n",
    "image_path = \"Vulture_crop.tif\"\n",
    "crops = preprocess.split_raster(\n",
    "    path_to_raster=image_path,\n",
    "    patch_size=1500\n",
    ")\n",
    "print(crops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the great label studio python SDK to create a new project. For those already using label-studio check out settings -> labeling-interface->code, to get a plain text representation of the labeling environment. This is super useful for making duplicates of existing projects and tweaking the results in the browser. For the purposes of this blog post, let's just launch on our local machine. For your API key, set the env variable, \"LABEL_STUDIO_API_KEY\" in a .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     19\u001b[0m project \u001b[38;5;241m=\u001b[39m ls\u001b[38;5;241m.\u001b[39mstart_project(\n\u001b[1;32m     20\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabelStudio SDK Example\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     label_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Add the images to the project\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcrops\u001b[49m:\n\u001b[1;32m     32\u001b[0m     project\u001b[38;5;241m.\u001b[39madd_file(crop)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crops' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the SDK and the client module\n",
    "from label_studio_sdk import Client\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "for line in open(\".env\"):\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    os.environ[key] = value\n",
    "\n",
    "# Define the URL where Label Studio is accessible and the API key for your user account\n",
    "LABEL_STUDIO_URL = 'http://localhost:8080'\n",
    "API_KEY = os.environ['LABEL_STUDIO_API_KEY']\n",
    "\n",
    "# Connect to the Label Studio API and check the connection\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "\n",
    "project = ls.start_project(\n",
    "    title='LabelStudio SDK Example',\n",
    "    label_config='''\n",
    "    <View>\n",
    "    <Image name=\"image\" value=\"$image\" zoom=\"true\" zoomControl=\"true\" rotateControl=\"true\"/>\n",
    "    <RectangleLabels name=\"label\" toName=\"image\">        \n",
    "    <Label value=\"Great Egret\" background=\"#FFA39E\"/><Label value=\"Great Blue Heron\" background=\"#D4380D\"/><Label value=\"Wood Stork\" background=\"#FFC069\"/><Label value=\"Snowy Egret\" background=\"#AD8B00\"/><Label value=\"Anhinga\" background=\"#D3F261\"/><Label value=\"Unidentified White\" background=\"#389E0D\"/><Label value=\"White Ibis\" background=\"#5CDBD3\"/><Label value=\"Nest\" background=\"#FFA39E\"/><Label value=\"Help me!\" background=\"#D4380D\"/><Label value=\"Eggs\" background=\"#FFA39E\"/><Label value=\"Roseate Spoonbill\" background=\"#FFA39E\"/></RectangleLabels>\n",
    "    </View>\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Add the images to the project\n",
    "for crop in crops:\n",
    "    project.add_file(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting existing annotations from geospatial to image coordinates\n",
    "\n",
    "What if we already have geospatial annotations? Very often, members of team will use common geospatial tools, such as ArcGIS or QGIS to draw annotations on to data products. One potential hurdle is transforming these geospatial coordinates into formats more commonly used by machine learning models. For example, to convert our existing annotations into the pixel coordinate plane, we want the 0,0 origin to be the top left corner, and the coordinates to be the row, column location. By providing the annotations and the corresponding RGB image, we can translate the geospatial to be expressed as pixels (row, columns). Note this an approximation that doesn't account for the curvature of the earth and is only useful on small spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((6876.169 28793.212, 6876.169 28729.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           geometry\n",
       "0  None  POLYGON ((6876.169 28793.212, 6876.169 28729.4..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "existing_annotation_shp = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/labelstudio/existing_annotation.shp\"\n",
    "image_path = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/Raw_versus_Ortho/Vulture_03_28_2022.tif\"\n",
    "gdf = gpd.read_file(existing_annotation_shp)\n",
    "with rio.open(image_path) as src:\n",
    "    left, bottom, right, top = src.bounds\n",
    "    image_resolution = src.res[0]\n",
    "left, bottom, right, top = src.bounds\n",
    "image_resolution = src.res[0]\n",
    "\n",
    "gdf.geometry = gdf.geometry.translate(xoff=-left, yoff=-top)\n",
    "gdf.geometry = gdf.geometry.scale(xfact=1/image_resolution, yfact=-1/image_resolution, origin=(0,0))\n",
    "gdf.crs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation models for pre-annotation makes labeling faster.\n",
    "\n",
    "Now that we are ready to annotate, we find that fully annotating even the tiled data still takes a long time, since we spend alot of time zooming and panning to find all the birds. This is where the recent growth in open-source machine learning models makes all the difference. By leveraging existing broad classifiers we can pre-annotate our label studio workflow with 'Bird' label. See [DeepForest docs]() for more on these foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 3 Simple upload of DeepForest annotations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After humans have reviewed and corrected these annotations, we can download them to train machine learning models using the web interface or label-studio’s python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial predictions from image-coordinates\n",
    "\n",
    "Once we’ve made out predictions, we need to convert them back into geospatial coordinates to match our original data. This means going from boxes with image-based coordinates relative to the 0,0 origin to boxes with geospatial coordinates relative to the native map projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(rgb_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    left, bottom, right, top = bounds\n",
    "    pixelSizeX, pixelSizeY = dataset.res\n",
    "    crs = dataset.crs\n",
    "\n",
    "xmin_coords, ymin_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.miny,\n",
    "                                xs            cols=coordinates.minx,\n",
    "                                            offset='center')\n",
    "\n",
    "xmax_coords, ymax_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.maxy,\n",
    "                                            cols=coordinates.maxx,\n",
    "                                            offset='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final raster with geospatial predictions]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make this faster and more accurate?\n",
    "\n",
    "Label studio’s open-source API opens the possibility for community driven development. We need your help! There are many ways to make annotation faster, which means more data, better models, and better information to make conservation decisions. We would welcome developers looking to make a difference to help make label-studio more ready for geospatial data.\n",
    "\n",
    "1.\tAllow overlays of multiple products. Often, we have several data types, such as RGB data, airborne multi-spectral data and LiDAR data that we want to look at in unison to help guide annotation. While this is possible in tools like QGIS, it is not natively doable in label-studio. \n",
    "2.\tIntelligent zooming to annotations. Double clicking on the UI should zoom to a particular annotation, reducing costly scrolling time among objects. \n",
    "3. Improved backend integrations! The future of data labeling is interactive collaboration with foundation machine learning models. In the example above we preprocessed an image with a model and served the result to a person. An even better strategy is to take input from a person, then pass it to a model, and return it. This is starting to make its way into label-studio, with Meta's Segment-Anything model. A flavor of this model, called SAM-geo has been trained for remote sensing, and can be directly integrated into the server backend to provide segmentations based on prompts. These predicted annotations can then be quickly cleaned up by a human at a fraction of the total time needed to create polygon annotations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowlegements\n",
    "\n",
    "The everglades bird survey gratefully acknowledges support from the South Florida Water Management District, US Army Corps of Engineers and University of Florida. Lindsey Gardner flew the UAV mission and Henry Senyondo processed the orthomosaic data. The DeepForest python package was supported by the National Science Foundation. The Weecology lab has no conflicts of interest or funding  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTreeAttention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
