{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using label-studio for rapid annotation of geospatial data for airborne environmental monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environmental remote sensing using airborne image capture allows unprecedented insight into the distribution and abundance of organisms on earth. To detect and classify objects in airborne imagery, machine learning tools need to label the location and classes of objects. Here at the Weecology Lab at the University of Florida, we use label-studio to annotate airborne imagery for a variety of geospatial tasks, including tree detection in very high resolution orthophotos, and bird classification in UAV orthomosaics. Label-studio’s web interface has been key in creating labels for our machine learning workflow and keep our distributed team on the same page. Machine learning models for airborne object detection and classification require thousands of labels to create accurate detection models. In this post, I provide some key steps and uses for geospatial data labeling in label studio.\n",
    "\n",
    "1. Converting large geospatial tiles into smaller crops in a format compatible with label-studio?\n",
    "2. Creating a bounding box annotation project using the label-studio python-sdk\n",
    "3. Placing pre-annotated images into label-studio using existing foundation models\n",
    "\n",
    "The data comes from our work surveying bird colonies to support everglades restoration efforts using Unpiloted Aerial Vehivles (UAV). Let’s look at an orthomosaic image of one of those colonies. We also make use of our [DeepForest python package](https://deepforest.readthedocs.io/en/latest/), a tool for airborne model training focused on ecological monitoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1](public/Figure1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* A working version of label-studio up and running. For this tutorial we will be serving local studio, start label studio with local\n",
    "label-studio, see below for making sure to set proper env variables for local serving.\n",
    "\n",
    "Here is a video of me doing that!\n",
    "\n",
    "**_NOTE:_**  We are serving local files. Label-studio is generally setup to use cloud storage and you need to allow local file serving just for this tutorial, see https://labelstud.io/guide/start#Set_environment_variables. I found this process a little frustrating, so I made a video. \n",
    "\n",
    "<div>\n",
    "    <a href=\"https://www.loom.com/share/18017f925b054f6d9290622be7b559f9\">\n",
    "      <p>Starting Label Studio with Local File Storage - Watch Video</p>\n",
    "    </a>\n",
    "    <a href=\"https://www.loom.com/share/18017f925b054f6d9290622be7b559f9\">\n",
    "      <img style=\"max-width:300px;\" src=\"https://cdn.loom.com/sessions/thumbnails/18017f925b054f6d9290622be7b559f9-with-play.gif\">\n",
    "    </a>\n",
    "  </div>\n",
    "  \n",
    "* pip install DeepForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing geospatial data for labeling\n",
    "\n",
    "### Tiling orthomosaics into manageable pieces\n",
    "\n",
    "This orthomosaic has been georectified, meaning that the pixels in the imagery correspond to locations on the earth’s surface. While this is crucial for matching images among surveys, or associating images with other data products, it presents a couple challenges for standard annotation workflows. Geospatial data products are often very large, making them difficult to render and awkward to work with. Tiling the image into smaller pieces can be quite useful. Using our DeepForest python package, we have a couple helpful utility functions. These are mostly wrappers from common python packages for geospatial data, like Rasterio and Geopandas, which deserve a lot of credit in helping make geospatial data easy to use in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data, its hard to define a generic tool here, see curl, wget, or requests for more complex examples\n",
    "import os\n",
    "download_path = \"https://www.dropbox.com/scl/fi/f60hv8qzm7eyuw0tbdz7g/vulture_crop.tif?rlkey=rax4403x53r9ifqg5uq3ce67o&dl=1\"\n",
    "\n",
    "if not os.path.exists(\"Vulture_crop.tif\"):\n",
    "    os.system(\"curl -L -o Vulture_crop.tif https://www.dropbox.com/scl/fi/f60hv8qzm7eyuw0tbdz7g/vulture_crop.tif?rlkey=rax4403x53r9ifqg5uq3ce67o&dl=1\")\n",
    "\n",
    "#pip install deepforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crops/Vulture_crop_0.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from deepforest import preprocess\n",
    "image_path = \"Vulture_crop.tif\"\n",
    "try:\n",
    "    os.mkdir(\"crops\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "crops = preprocess.split_raster(\n",
    "    path_to_raster=image_path,\n",
    "    save_dir=\"crops\",\n",
    "    patch_size=1500\n",
    ")\n",
    "print(crops[0])\n",
    "\n",
    "# For the sake of this tutorial, we will only use the first five crops\n",
    "crops = crops[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Create a local label-studio project\n",
    "Let's use the label studio python SDK to create a new project. For those already using label-studio check out settings -> labeling-interface->code, to get a plain text representation of the labeling environment. This is super useful for making duplicates of existing projects and tweaking the results in the browser. For the purposes of this blog post, let's just launch on our local machine. For your API key, set the env variable, \"LABEL_STUDIO_API_KEY\" in a .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SDK and the client module\n",
    "from label_studio_sdk import Client\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for line in open(\".env\"):\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    os.environ[key] = value\n",
    "\n",
    "#Start up label-studio if needed\n",
    "#os.system(\"label-studio\")    \n",
    "\n",
    "# Define the URL where Label Studio is accessible and the API key for your user account, these are the typical defaults, but you need to check the output of the above line.\n",
    "LABEL_STUDIO_URL = 'http://localhost:8080'\n",
    "API_KEY = os.environ['LABEL_STUDIO_API_KEY']\n",
    "\n",
    "# Connect to the Label Studio API and check the connection\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "\n",
    "# Start a new project or get an existing one\n",
    "projects = ls.get_projects()\n",
    "existing_project = [x for x in projects if x.title == \"LabelStudio SDK Example\"]\n",
    "if len(existing_project) > 0:\n",
    "    existing_project = existing_project[0]\n",
    "else:\n",
    "    project = ls.start_project(\n",
    "        title='LabelStudio SDK Example',\n",
    "        label_config='''\n",
    "        <View>\n",
    "        <Image name=\"image\" value=\"$image\" zoom=\"true\" zoomControl=\"true\" rotateControl=\"true\"/>\n",
    "        <RectangleLabels name=\"label\" toName=\"image\">        \n",
    "        <Label value=\"Great Egret\" background=\"#FFA39E\"/><Label value=\"Great Blue Heron\" background=\"#D4380D\"/><Label value=\"Wood Stork\" background=\"#FFC069\"/><Label value=\"Snowy Egret\" background=\"#AD8B00\"/><Label value=\"Anhinga\" background=\"#D3F261\"/><Label value=\"Unidentified White\" background=\"#389E0D\"/><Label value=\"White Ibis\" background=\"#5CDBD3\"/><Label value=\"Nest\" background=\"#FFA39E\"/><Label value=\"Help me!\" background=\"#D4380D\"/><Label value=\"Eggs\" background=\"#FFA39E\"/><Label value=\"Roseate Spoonbill\" background=\"#FFA39E\"/></RectangleLabels>\n",
    "        </View>\n",
    "        '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cropped images to be annotated\n",
    "\n",
    "There are several ways to add annotations to label-studio. This step can be quite finicky when trying to match the [label-studio format](https://labelstud.io/guide/tasks#Basic-Label-Studio-JSON-format). The easiest solution I find is to go to the GUI and add a storage location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some users, this will be a good start, we can now go to the project and begin annotating the classes that were in our project string above. But what if we want to go further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation models for pre-annotation makes labeling faster.\n",
    "\n",
    "Now that we are ready to annotate, we find that fully annotating even the tiled data still takes a long time, since we spend alot of time zooming and panning to find all the birds. This is where the recent growth in open-source machine learning models makes all the difference. By leveraging existing broad classifiers we can pre-annotate our label studio workflow with 'Bird' bounding box. See [DeepForest docs](https://deepforest.readthedocs.io/en/latest/prebuilt.html#bird-detection) for more on these foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/deepforest/data/deepforest_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation file provided. Turning off validation loop\n",
      "Model from BirdDetector Repo release https://github.com/weecology/BirdDetector/releases/tag/1.1 was already downloaded. Loading model from file.\n",
      "Loading pre-built model: https://github.com/weecology/BirdDetector/releases/tag/1.1\n",
      "Setting default score threshold to 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 16/16 [01:02<00:00,  0.26it/s]\n",
      "39 predictions in overlapping windows, applying non-max supression\n",
      "37 predictions kept after non-max suppression\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PngImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m boxes\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m     14\u001b[0m img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(crops[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m example_plot \u001b[38;5;241m=\u001b[39m \u001b[43mplot_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/deepforest/visualize.py:117\u001b[0m, in \u001b[0;36mplot_predictions\u001b[0;34m(image, df, color, thickness)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_predictions\u001b[39m(image, df, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot a set of boxes on an image\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    By default this function does not show, but only plots an axis\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    Label column must be numeric!\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m        image: a numpy array with drawn annotations\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    118\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput images must be channels last format [h, w, 3] not channels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst [3, h, w], using np.rollaxis(image, 0, 3) to invert!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m         image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(image, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PngImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#Code Block 3 Simple upload of DeepForest annotations. \n",
    "from deepforest import main\n",
    "from deepforest.visualize import plot_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "m = main.deepforest()\n",
    "m.use_bird_release()\n",
    "\n",
    "#Show an example\n",
    "boxes = m.predict_tile(crops[0])\n",
    "boxes.head()\n",
    "img = np.array(PIL.Image.open(crops[0]))\n",
    "\n",
    "example_plot = plot_predictions(img, boxes)\n",
    "plt.show()\n",
    "\n",
    "predictions = []\n",
    "for crop in crops:\n",
    "    prediction = m.predict_tile(crop)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to label-studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/deepforest/data/deepforest_config.yml\n",
      "No validation file provided. Turning off validation loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from BirdDetector Repo release https://github.com/weecology/BirdDetector/releases/tag/1.1 was already downloaded. Loading model from file.\n",
      "Loading pre-built model: https://github.com/weecology/BirdDetector/releases/tag/1.1\n",
      "Setting default score threshold to 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 16/16 [00:58<00:00,  0.27it/s]\n",
      "39 predictions in overlapping windows, applying non-max supression\n",
      "37 predictions kept after non-max suppression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/Users/benweinstein/.conda/envs/labelstudio_blog/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 16/16 [01:00<00:00,  0.27it/s]\n",
      "39 predictions in overlapping windows, applying non-max supression\n",
      "37 predictions kept after non-max suppression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_dataframe_to_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mpredict_tile(crop)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Upload the predictions to Label Studio\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m json_string \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_dataframe_to_json\u001b[49m(predictions)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreannotations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     52\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(json_string)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_dataframe_to_json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def label_studio_bbox_format(root_dir, preannotations):\n",
    "    \"\"\"Create a JSON string for a single image the Label Studio API.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    original_width = PIL.Image.open(os.path.join(root_dir,os.path.basename(preannotations.image_path.unique()[0]))).size[0]\n",
    "    original_height = PIL.Image.open(os.path.join(root_dir,os.path.basename(preannotations.image_path.unique()[0]))).size[1]\n",
    "\n",
    "    for index, row in preannotations.iterrows():\n",
    "        result = {\n",
    "            \"value\":{\n",
    "                \"x\": row['xmin']/original_width*100,\n",
    "                \"y\": row['ymin']/original_height*100,\n",
    "                \"width\": (row['xmax'] - row['xmin'])/original_width*100,\n",
    "                \"height\": (row['ymax'] - row['ymin'])/original_height*100,\n",
    "                \"rotation\": 0,\n",
    "                \"rectanglelabels\": [row[\"label\"]]\n",
    "            },\n",
    "            \"score\": row[\"score\"],\n",
    "            \"to_name\": \"image\",\n",
    "            \"type\": \"rectanglelabels\",\n",
    "            \"from_name\": \"label\",\n",
    "            \"original_width\": original_width,\n",
    "            \"original_height\": original_height\n",
    "        }\n",
    "        predictions.append(result)\n",
    "    # As a dict\n",
    "    return {\"result\": predictions}\n",
    "\n",
    "def import_image_tasks(label_studio_project,images,predictions,root_dir=None):\n",
    "    \"\"\"Upload images and predictions to a Label Studio project.\n",
    "    Args:\n",
    "    label_studio_project: A label studio project object\n",
    "    images: A list of image names\n",
    "    predictions: A list of DeepForest predictions\n",
    "    root_dir: The root directory of the images\n",
    "    \"\"\"\n",
    "\n",
    "    # Get project\n",
    "    tasks = []\n",
    "    for image,prediction in zip(images, predictions):\n",
    "        data_dict = {'image': image}\n",
    "        if predictions:\n",
    "            #Skip predictions if there are none\n",
    "            if prediction.empty:\n",
    "                result_dict = []\n",
    "            else:\n",
    "                result_dict = [label_studio_bbox_format(root_dir, prediction)]\n",
    "            upload_dict = {\"data\":data_dict, \"predictions\":result_dict}\n",
    "        tasks.append(upload_dict)\n",
    "    label_studio_project.import_tasks(tasks)\n",
    "\n",
    "# Use our helper function to upload the predictions and images\n",
    "import_image_tasks(\n",
    "    label_studio_project=existing_project,\n",
    "    image_names = [os.path.basename(crop) for crop in crops],\n",
    "    predictions=[m.predict_tile(crop) for crop in crops],\n",
    "    root_dir=\"crops\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After humans have reviewed and corrected these annotations, we can download them to train machine learning models using the web interface or label-studio’s python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting among geospatial and image-based coordinates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting existing annotations from geospatial to image coordinates\n",
    "\n",
    "What if we already have geospatial annotations? Very often, members of team will use common geospatial tools, such as ArcGIS or QGIS to draw annotations on to data products. One potential hurdle is transforming these geospatial coordinates into formats more commonly used by machine learning models. For example, to convert our existing annotations into the pixel coordinate plane, we want the 0,0 origin to be the top left corner, and the coordinates to be the row, column location. By providing the annotations and the corresponding RGB image, we can translate the geospatial to be expressed as pixels (row, columns). Note this an approximation that doesn't account for the curvature of the earth and is only useful on small spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "existing_annotation_shp = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/labelstudio/existing_annotation.shp\"\n",
    "image_path = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/Raw_versus_Ortho/Vulture_03_28_2022.tif\"\n",
    "gdf = gpd.read_file(existing_annotation_shp)\n",
    "with rio.open(image_path) as src:\n",
    "    left, bottom, right, top = src.bounds\n",
    "    image_resolution = src.res[0]\n",
    "left, bottom, right, top = src.bounds\n",
    "image_resolution = src.res[0]\n",
    "\n",
    "gdf.geometry = gdf.geometry.translate(xoff=-left, yoff=-top)\n",
    "gdf.geometry = gdf.geometry.scale(xfact=1/image_resolution, yfact=-1/image_resolution, origin=(0,0))\n",
    "gdf.crs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial predictions from image-coordinates\n",
    "\n",
    "Once we’ve made out predictions, we need to convert them back into geospatial coordinates to match our original data. This means going from boxes with image-based coordinates relative to the 0,0 origin to boxes with geospatial coordinates relative to the native map projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(rgb_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    left, bottom, right, top = bounds\n",
    "    pixelSizeX, pixelSizeY = dataset.res\n",
    "    crs = dataset.crs\n",
    "\n",
    "xmin_coords, ymin_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.miny,\n",
    "                                xs            cols=coordinates.minx,\n",
    "                                            offset='center')\n",
    "\n",
    "xmax_coords, ymax_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.maxy,\n",
    "                                            cols=coordinates.maxx,\n",
    "                                            offset='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final raster with geospatial predictions]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make this faster and more accurate?\n",
    "\n",
    "Label studio’s open-source API opens the possibility for community driven development. We need your help! There are many ways to make annotation faster, which means more data, better models, and better information to make conservation decisions. We would welcome developers looking to make a difference to help make label-studio more ready for geospatial data.\n",
    "\n",
    "1.\tAllow overlays of multiple products. Often, we have several data types, such as RGB data, airborne multi-spectral data and LiDAR data that we want to look at in unison to help guide annotation. While this is possible in tools like QGIS, it is not natively doable in label-studio. \n",
    "2.\tIntelligent zooming to annotations. Double clicking on the UI should zoom to a particular annotation, reducing costly scrolling time among objects. \n",
    "3. Improved backend integrations! The future of data labeling is interactive collaboration with foundation machine learning models. In the example above we preprocessed an image with a model and served the result to a person. An even better strategy is to take input from a person, then pass it to a model, and return it. This is starting to make its way into label-studio, with Meta's Segment-Anything model. A flavor of this model, called SAM-geo has been trained for remote sensing, and can be directly integrated into the server backend to provide segmentations based on prompts. These predicted annotations can then be quickly cleaned up by a human at a fraction of the total time needed to create polygon annotations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowlegements\n",
    "\n",
    "The everglades bird survey gratefully acknowledges support from the South Florida Water Management District, US Army Corps of Engineers and University of Florida. Lindsey Gardner flew the UAV mission and Henry Senyondo processed the orthomosaic data. The DeepForest python package was supported by the National Science Foundation. The Weecology lab has no conflicts of interest or funding  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTreeAttention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
