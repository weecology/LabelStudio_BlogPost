{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using label-studio for rapid annotation of geospatial data for airborne environmental monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environmental remote sensing using airborne image capture allows unprecedented insight into the distribution and abundance of organisms on earth. To detect and classify objects in airborne imagery, machine learning tools need to label the location and classes of objects. Here at the Weecology Lab at the University of Florida, we use label-studio to annotate airborne imagery for a variety of geospatial tasks, including tree detection in very high resolution orthophotos, and bird classification in UAV orthomosaics. Label-studio’s web interface has been key in creating labels for our machine learning workflow and keep our distributed team on the same page. Machine learning models for airborne object detection and classification require thousands of labels to create accurate detection models. In this post, I provide some key steps and uses for geospatial data labeling in label studio.\n",
    "\n",
    "1. Converting large geospatial tiles into smaller crops in a format compatible with label-studio?\n",
    "2. Creating a bounding box annotation project using the label-studio python-sdk\n",
    "3. Placing pre-annotated images into label-studio using existing foundation models\n",
    "\n",
    "The data comes from our work surveying bird colonies to support everglades restoration efforts using Unpiloted Aerial Vehivles (UAV). Let’s look at an orthomosaic image of one of those colonies. We also make use of our [DeepForest python package](https://deepforest.readthedocs.io/en/latest/), a tool for airborne model training focused on ecological monitoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1](public/Figure1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* A working version of label-studio up and running. For this tutorial we will be serving local studio, start label studio with local\n",
    "label-studio, see below for making sure to set proper env variables for local serving.\n",
    "* pip install DeepForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing geospatial data for labeling\n",
    "\n",
    "### Tiling orthomosaics into manageable pieces\n",
    "\n",
    "This orthomosaic has been georectified, meaning that the pixels in the imagery correspond to locations on the earth’s surface. While this is crucial for matching images among surveys, or associating images with other data products, it presents a couple challenges for standard annotation workflows. Geospatial data products are often very large, making them difficult to render and awkward to work with. Tiling the image into smaller pieces can be quite useful. Using our DeepForest python package, we have a couple helpful utility functions. These are mostly wrappers from common python packages for geospatial data, like Rasterio and Geopandas, which deserve a lot of credit in helping make geospatial data easy to use in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   235    0   235    0     0    218      0 --:--:--  0:00:01 --:--:--   220\n",
      "100  239M  100  239M    0     0  27.9M      0  0:00:08  0:00:08 --:--:-- 36.2M\n"
     ]
    }
   ],
   "source": [
    "# Download sample data, its hard to define a generic tool here, see curl, wget, or requests for more complex examples\n",
    "import os\n",
    "download_path = \"https://www.dropbox.com/scl/fi/f60hv8qzm7eyuw0tbdz7g/vulture_crop.tif?rlkey=rax4403x53r9ifqg5uq3ce67o&dl=1\"\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    os.system(\"curl -L -o Vulture_crop.tif https://www.dropbox.com/scl/fi/f60hv8qzm7eyuw0tbdz7g/vulture_crop.tif?rlkey=rax4403x53r9ifqg5uq3ce67o&dl=1\")\n",
    "\n",
    "#pip install deepforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crops/Vulture_crop_0.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from deepforest import preprocess\n",
    "image_path = \"Vulture_crop.tif\"\n",
    "os.mkdir(\"crops\")\n",
    "crops = preprocess.split_raster(\n",
    "    path_to_raster=image_path,\n",
    "    save_dir=\"crops\",\n",
    "    patch_size=1500\n",
    ")\n",
    "print(crops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Create a local label-studio project\n",
    "Let's use the label studio python SDK to create a new project. For those already using label-studio check out settings -> labeling-interface->code, to get a plain text representation of the labeling environment. This is super useful for making duplicates of existing projects and tweaking the results in the browser. For the purposes of this blog post, let's just launch on our local machine. For your API key, set the env variable, \"LABEL_STUDIO_API_KEY\" in a .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SDK and the client module\n",
    "from label_studio_sdk import Client\n",
    "\n",
    "import os\n",
    "\n",
    "for line in open(\".env\"):\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    os.environ[key] = value\n",
    "\n",
    "#Start up label-studio if needed\n",
    "#os.system(\"label-studio\")    \n",
    "\n",
    "# Define the URL where Label Studio is accessible and the API key for your user account, these are the typical defaults, but you need to check the output of the above line.\n",
    "LABEL_STUDIO_URL = 'http://localhost:8082'\n",
    "API_KEY = os.environ['LABEL_STUDIO_API_KEY']\n",
    "\n",
    "# Connect to the Label Studio API and check the connection\n",
    "ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "ls.check_connection()\n",
    "\n",
    "project = ls.start_project(\n",
    "    title='LabelStudio SDK Example',\n",
    "    label_config='''\n",
    "    <View>\n",
    "    <Image name=\"image\" value=\"$image\" zoom=\"true\" zoomControl=\"true\" rotateControl=\"true\"/>\n",
    "    <RectangleLabels name=\"label\" toName=\"image\">        \n",
    "    <Label value=\"Great Egret\" background=\"#FFA39E\"/><Label value=\"Great Blue Heron\" background=\"#D4380D\"/><Label value=\"Wood Stork\" background=\"#FFC069\"/><Label value=\"Snowy Egret\" background=\"#AD8B00\"/><Label value=\"Anhinga\" background=\"#D3F261\"/><Label value=\"Unidentified White\" background=\"#389E0D\"/><Label value=\"White Ibis\" background=\"#5CDBD3\"/><Label value=\"Nest\" background=\"#FFA39E\"/><Label value=\"Help me!\" background=\"#D4380D\"/><Label value=\"Eggs\" background=\"#FFA39E\"/><Label value=\"Roseate Spoonbill\" background=\"#FFA39E\"/></RectangleLabels>\n",
    "    </View>\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cropped images to be annotated\n",
    "\n",
    "There are several ways to add annotations to label-studio. This step can be quite finicky when trying to match the [label-studio format](https://labelstud.io/guide/tasks#Basic-Label-Studio-JSON-format). The easiest solution I find is to go to the GUI and add a storage location. Here is a video of me doing that!\n",
    "\n",
    "**_NOTE:_**  We are serving local files. Label-studio is generally setup to use cloud storage and you need to allow local file serving just for this tutorial, see https://labelstud.io/guide/start#Set_environment_variables. I found this process a little frustrating, so I made a video. \n",
    "\n",
    "<div>\n",
    "    <a href=\"https://www.loom.com/share/18017f925b054f6d9290622be7b559f9\">\n",
    "      <p>Starting Label Studio with Local File Storage - Watch Video</p>\n",
    "    </a>\n",
    "    <a href=\"https://www.loom.com/share/18017f925b054f6d9290622be7b559f9\">\n",
    "      <img style=\"max-width:300px;\" src=\"https://cdn.loom.com/sessions/thumbnails/18017f925b054f6d9290622be7b559f9-with-play.gif\">\n",
    "    </a>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some users, this will be a good start, we can now go to the project and begin annotating the classes that were in our project string above. But what if we want to go further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation models for pre-annotation makes labeling faster.\n",
    "\n",
    "Now that we are ready to annotate, we find that fully annotating even the tiled data still takes a long time, since we spend alot of time zooming and panning to find all the birds. This is where the recent growth in open-source machine learning models makes all the difference. By leveraging existing broad classifiers we can pre-annotate our label studio workflow with 'Bird' bounding box. See [DeepForest docs](https://deepforest.readthedocs.io/en/latest/prebuilt.html#bird-detection) for more on these foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 3 Simple upload of DeepForest annotations. \n",
    "from deepforest import main\n",
    "m = main.deepforest()\n",
    "m.use_bird_release()\n",
    "\n",
    "#Show an example\n",
    "boxes = m.predict_tile(crops[0])\n",
    "boxes.head()\n",
    "\n",
    "for crop in crops:\n",
    "    predictions = m.predict_tile(crop)\n",
    "    project.upload_annotations(\n",
    "        file_path=crop,\n",
    "        label_config_id=project.label_config_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After humans have reviewed and corrected these annotations, we can download them to train machine learning models using the web interface or label-studio’s python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting among geospatial and image-based coordinates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting existing annotations from geospatial to image coordinates\n",
    "\n",
    "What if we already have geospatial annotations? Very often, members of team will use common geospatial tools, such as ArcGIS or QGIS to draw annotations on to data products. One potential hurdle is transforming these geospatial coordinates into formats more commonly used by machine learning models. For example, to convert our existing annotations into the pixel coordinate plane, we want the 0,0 origin to be the top left corner, and the coordinates to be the row, column location. By providing the annotations and the corresponding RGB image, we can translate the geospatial to be expressed as pixels (row, columns). Note this an approximation that doesn't account for the curvature of the earth and is only useful on small spatial scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "existing_annotation_shp = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/labelstudio/existing_annotation.shp\"\n",
    "image_path = \"/Users/benweinstein/Dropbox/Weecology/everglades_species/Raw_versus_Ortho/Vulture_03_28_2022.tif\"\n",
    "gdf = gpd.read_file(existing_annotation_shp)\n",
    "with rio.open(image_path) as src:\n",
    "    left, bottom, right, top = src.bounds\n",
    "    image_resolution = src.res[0]\n",
    "left, bottom, right, top = src.bounds\n",
    "image_resolution = src.res[0]\n",
    "\n",
    "gdf.geometry = gdf.geometry.translate(xoff=-left, yoff=-top)\n",
    "gdf.geometry = gdf.geometry.scale(xfact=1/image_resolution, yfact=-1/image_resolution, origin=(0,0))\n",
    "gdf.crs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial predictions from image-coordinates\n",
    "\n",
    "Once we’ve made out predictions, we need to convert them back into geospatial coordinates to match our original data. This means going from boxes with image-based coordinates relative to the 0,0 origin to boxes with geospatial coordinates relative to the native map projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(rgb_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    left, bottom, right, top = bounds\n",
    "    pixelSizeX, pixelSizeY = dataset.res\n",
    "    crs = dataset.crs\n",
    "\n",
    "xmin_coords, ymin_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.miny,\n",
    "                                xs            cols=coordinates.minx,\n",
    "                                            offset='center')\n",
    "\n",
    "xmax_coords, ymax_coords = rasterio.transform.xy(transform=dataset.transform,\n",
    "                                            rows=coordinates.maxy,\n",
    "                                            cols=coordinates.maxx,\n",
    "                                            offset='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final raster with geospatial predictions]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make this faster and more accurate?\n",
    "\n",
    "Label studio’s open-source API opens the possibility for community driven development. We need your help! There are many ways to make annotation faster, which means more data, better models, and better information to make conservation decisions. We would welcome developers looking to make a difference to help make label-studio more ready for geospatial data.\n",
    "\n",
    "1.\tAllow overlays of multiple products. Often, we have several data types, such as RGB data, airborne multi-spectral data and LiDAR data that we want to look at in unison to help guide annotation. While this is possible in tools like QGIS, it is not natively doable in label-studio. \n",
    "2.\tIntelligent zooming to annotations. Double clicking on the UI should zoom to a particular annotation, reducing costly scrolling time among objects. \n",
    "3. Improved backend integrations! The future of data labeling is interactive collaboration with foundation machine learning models. In the example above we preprocessed an image with a model and served the result to a person. An even better strategy is to take input from a person, then pass it to a model, and return it. This is starting to make its way into label-studio, with Meta's Segment-Anything model. A flavor of this model, called SAM-geo has been trained for remote sensing, and can be directly integrated into the server backend to provide segmentations based on prompts. These predicted annotations can then be quickly cleaned up by a human at a fraction of the total time needed to create polygon annotations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowlegements\n",
    "\n",
    "The everglades bird survey gratefully acknowledges support from the South Florida Water Management District, US Army Corps of Engineers and University of Florida. Lindsey Gardner flew the UAV mission and Henry Senyondo processed the orthomosaic data. The DeepForest python package was supported by the National Science Foundation. The Weecology lab has no conflicts of interest or funding  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTreeAttention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
